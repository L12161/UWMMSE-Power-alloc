{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "in_tf2_no py implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO6sepu1GxpqljzIQ2YHMzx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/L12161/UWMMSE-Power-alloc/blob/main/in_tf2_no_py_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA GENERATION SECTION"
      ],
      "metadata": {
        "id": "rx3h6r2NooHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow==1.14.0\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5Nu0jriolYD",
        "outputId": "5a1f7dc9-1ec9-4177-d1a9-3cd298c05af7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "emzLQtp2JJU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import sys\n",
        "# import pdb      # debugg korte help korbe ; aro jante:  https://realpython.com/python-debugging-pdb/\n",
        "# import pickle   # python object serializer. Eta diee ami kono obj, list, table ke file hishebe save kore felte parbo\n",
        "# # aro jante : https://docs.python.org/3/library/pickle.html \n",
        "# import numpy as np\n"
      ],
      "metadata": {
        "id": "09a_HnFaJsoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "SOURCE_PATH = '/content/SISO_DATASET'\n",
        "DESTINATION_PATH = '/content/gdrive/My Drive/THESIS DATASETS'\n",
        "shutil.move(SOURCE_PATH, DESTINATION_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GREzmxEkGldM",
        "outputId": "a12644f9-0b3a-4926-e5e7-6933b756768e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/THESIS DATASETS/SISO_DATASET'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transmitters_fix = transmitters\n",
        "receivers_fix = receivers"
      ],
      "metadata": {
        "id": "VxbFwARcCcRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "nNodes = 20 \n",
        "pl = 2.2\n",
        "alpha = 1\n",
        "r=1 \n",
        "\n",
        "# transmitters = np.random.uniform(low=-nNodes/r, high=nNodes/r, size=(nNodes,2))\n",
        "# receivers = transmitters + np.random.uniform(low=-nNodes/4,high=nNodes/4, size=(nNodes,2))\n",
        "transmitters = transmitters_fix\n",
        "receivers = receivers_fix\n",
        "L = np.zeros((nNodes,nNodes))\n",
        "\n",
        "for i in np.arange(nNodes):\n",
        "  for j in np.arange(nNodes):\n",
        "    d = np.linalg.norm(transmitters[i,:]-receivers[j,:])\n",
        "    L[i,j] = np.power(d,-pl)\n"
      ],
      "metadata": {
        "id": "dmGTVNED8Upu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Datagen.py\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pdb\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# Eperiment\n",
        "dataID = 'nNode_25_pl_2p2'\n",
        "\n",
        "# Number of nodes\n",
        "nNodes = 25\n",
        "\n",
        "# Path gain exponent\n",
        "pl = 2.2\n",
        "\n",
        "# Rayleigh distribution scale\n",
        "alpha = 1 \n",
        "\n",
        "# Batch size\n",
        "batch_size = 64\n",
        "\n",
        "# Training iterations\n",
        "tr_iter = 10000\n",
        "\n",
        "# Testing iterations\n",
        "te_iter = 100\n",
        "\n",
        "\n",
        "# Build random geometric graph\n",
        "def build_adhoc_network( nNodes, r=1, pl=2.2 ):\n",
        "    transmitters = np.random.uniform(low=-nNodes/r, high=nNodes/r, size=(nNodes,2))\n",
        "    receivers = transmitters + np.random.uniform(low=-nNodes/4,high=nNodes/4, size=(nNodes,2))\n",
        "    # transmitters = transmitters_fix\n",
        "    # receivers = receivers_fix\n",
        "\n",
        "    L = np.zeros((nNodes,nNodes))\n",
        "\n",
        "    for i in np.arange(nNodes):\n",
        "        for j in np.arange(nNodes):\n",
        "            d = np.linalg.norm(transmitters[i,:]-receivers[j,:])\n",
        "            L[i,j] = np.power(d,-pl)\n",
        "\n",
        "    return( dict(zip(['tx', 'rx'],[transmitters, receivers] )), L )\n",
        "\n",
        "# Simulate Fading\n",
        "def sample_graph(batch_size, A, nNodes, alpha=1.):\n",
        "    samples = np.random.rayleigh(alpha, (batch_size, nNodes, nNodes))\n",
        "    #samples = (samples + np.transpose(samples,(0,2,1)))/2\n",
        "    PP = samples[None,:,:] * A\n",
        "    return PP[0]\n",
        "\n",
        "# Training Data\n",
        "def generate_data(batch_size, alpha, A, nNodes):\n",
        "    tr_H = []\n",
        "    te_H = []\n",
        "    \n",
        "    for indx in range(tr_iter):\n",
        "        # sample training data \n",
        "        H = sample_graph(batch_size, A, nNodes, alpha )\n",
        "        tr_H.append( H )\n",
        "\n",
        "    for indx in range(te_iter):\n",
        "        # sample test data \n",
        "        H = sample_graph(batch_size, A, nNodes, alpha )\n",
        "        te_H.append( H )\n",
        "\n",
        "    return( dict(zip(['train_H', 'test_H'],[tr_H, te_H] ) ) )\n",
        "        \n",
        "def main():\n",
        "    coord, A = build_adhoc_network( nNodes )\n",
        "    \n",
        "    # Create data path\n",
        "    if not os.path.exists('data/'+dataID):\n",
        "        os.makedirs('data/'+dataID)\n",
        "\n",
        "    # Coordinates of nodes\n",
        "    f = open('data/'+dataID+'/coordinates.pkl', 'wb')  \n",
        "    pickle.dump(coord, f)         \n",
        "    f.close()\n",
        "\n",
        "    # Geometric graph\n",
        "    f = open('data/'+dataID+'/A.pkl', 'wb')  \n",
        "    pickle.dump(A, f)          \n",
        "    f.close()\n",
        "    \n",
        "    # Training data\n",
        "    data = generate_data(batch_size, alpha, A, nNodes)\n",
        "    f = open('data/'+dataID+'/H.pkl', 'wb')  \n",
        "    pickle.dump(data, f)         \n",
        "    f.close()\n",
        "\n",
        "    \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    rn = np.random.randint(2**20)\n",
        "    rn1 = np.random.randint(2**20)\n",
        "    np.random.seed(rn1)\n",
        "\n",
        "    main()"
      ],
      "metadata": {
        "id": "vNYZwvj5mjHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# coord, A_dummy = build_adhoc_network( nNodes )\n",
        "# H_hopefullyCH = sample_graph(64, A_dummy, nNodes, 1 )\n",
        "# len(H_hopefullyCH)"
      ],
      "metadata": {
        "id": "8VWrA4sXAZC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model gen"
      ],
      "metadata": {
        "id": "73m0W8WhqeIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pdb\n",
        "import tensorflow as tf\n",
        "\n",
        "# UWMMSE\n",
        "class UWMMSE(object):\n",
        "        # Initialize\n",
        "        def __init__( self, Pmax=1., var=7e-10, feature_dim=3, batch_size=64, layers=4, learning_rate=1e-3, max_gradient_norm=5.0, exp='uwmmse' ):\n",
        "            self.Pmax              = tf.cast( Pmax, tf.float64 )\n",
        "            self.var               = var\n",
        "            self.feature_dim       = feature_dim\n",
        "            self.batch_size        = batch_size\n",
        "            self.layers            = layers\n",
        "            self.learning_rate     = learning_rate\n",
        "            self.max_gradient_norm = max_gradient_norm\n",
        "            self.exp               = exp\n",
        "            self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
        "            self.build_model()\n",
        "\n",
        "        # Build Model\n",
        "        def build_model(self):\n",
        "            self.init_placeholders()\n",
        "            self.build_network()\n",
        "            self.build_objective()\n",
        "            \n",
        "        def init_placeholders(self):\n",
        "            # CSI [Batch_size X Nodes X Nodes]\n",
        "            self.H = tf.compat.v1.placeholder(tf.float64, shape=[None, None, None], name=\"H\")\n",
        "            \n",
        "            # NSI [Batch_size X Nodes X Features]\n",
        "            #self.x = tf.compat.v1.placeholder(tf.float64, shape=[None, None, self.feature_dim], name=\"x\")\n",
        "            \n",
        "            # Node Weights [Batch_size X Nodes]\n",
        "            #self.alpha = tf.compat.v1.placeholder(tf.float64, shape=[None, None], name=\"alpha\")\n",
        "            \n",
        "            # Boolean for Training/Inference \n",
        "            #self.phase = tf.compat.v1.placeholder_with_default(False, shape=(), name='phase')\n",
        "        \n",
        "        \n",
        "        # Building network\n",
        "        def build_network(self):\n",
        "            # Squared H \n",
        "            self.Hsq = tf.math.square(self.H)\n",
        "            \n",
        "            # Diag H\n",
        "            dH =  tf.linalg.diag_part( self.H ) \n",
        "            self.dH = tf.linalg.diag( dH )\n",
        "            \n",
        "            # Retrieve number of nodes for initializing V\n",
        "            self.nNodes = tf.shape( self.H )[-1]\n",
        "\n",
        "            # Maximum V = sqrt(Pmax)\n",
        "            Vmax = tf.math.sqrt(self.Pmax)\n",
        "\n",
        "            # Initial V\n",
        "            V = Vmax * tf.ones([self.batch_size, self.nNodes], dtype=tf.float64)\n",
        "            \n",
        "            self.pow_alloc = []\n",
        "            \n",
        "            # Iterate over layers l\n",
        "            for l in range(self.layers):\n",
        "                with tf.compat.v1.variable_scope('Layer{}'.format(l+1)):\n",
        "                    # Compute U^l\n",
        "                    U = self.U_block( V )\n",
        "                    \n",
        "                    # Compute W^l\n",
        "                    W_wmmse = self.W_block( U, V )\n",
        "                    \n",
        "                    # Learn a^l\n",
        "                    #print(\"a = \")\n",
        "                    a = self.gcn('a')\n",
        "                    #print(tf.shape(a))\n",
        "                    #a_value = a.eval(session=tf.compat.v1.Session())\n",
        "                    #print(a_value)\n",
        "\n",
        "                    # Learn b^l\n",
        "                    #print(\"b = \")\n",
        "                    b = self.gcn('b')\n",
        "                    #print(tf.shape(b))\n",
        "                    #b_value = b.eval(session=tf.compat.v1.Session())\n",
        "                    #print(b_value)\n",
        "\n",
        "                    # Compute Wcap^l = a^l * W^l + b^l\n",
        "                    W = tf.math.add( tf.math.multiply( a, W_wmmse ), b )\n",
        "                    \n",
        "                    # Learn mu^l\n",
        "                    mu = tf.compat.v1.get_variable( name='mu', initializer=tf.constant(0., shape=(), dtype=tf.float64))\n",
        "\n",
        "                    # Compute V^l\n",
        "                    if self.exp == 'wmmse':\n",
        "                        V = self.V_block( U, W_wmmse, 0. )\n",
        "                    else:\n",
        "                        V = self.V_block( U, W, mu )\n",
        "                    \n",
        "                    # Saturation non-linearity  ->  0 <= V <= Vmax\n",
        "                    V = tf.math.minimum(V, Vmax) + tf.math.maximum(V, 0) - V\n",
        "\n",
        "            # Final V\n",
        "            self.pow_alloc = V\n",
        "        \n",
        "        def U_block(self, V):\n",
        "            # H_ii * v_i\n",
        "            num = tf.math.multiply( tf.linalg.diag_part(self.H), V )\n",
        "            \n",
        "            # sigma^2 + sum_j( (H_ji)^2 * (v_j)^2 )\n",
        "            den = tf.reshape( tf.matmul( tf.transpose( self.Hsq, perm=[0,2,1] ), tf.reshape( tf.math.square( V ), [-1, self.nNodes, 1] ) ), [-1, self.nNodes] ) + self.var \n",
        "            \n",
        "            # U = num/den\n",
        "            return( tf.math.divide( num, den ) )\n",
        "\n",
        "        # Sum-rate = z\n",
        "        def W_block(self, U, V):\n",
        "            # 1 - u_i * H_ii * v_i\n",
        "            den = 1. - tf.math.multiply( tf.linalg.diag_part(self.H), tf.math.multiply( U, V ) )\n",
        "            \n",
        "            # W = 1/den\n",
        "            return( tf.math.reciprocal( den ) )\n",
        "\n",
        "        # Weighted Sum-rate = a * z\n",
        "        def W_block1(self, U, V):\n",
        "            # 1 - u_i * H_ii * v_i\n",
        "            den = 1. - tf.math.multiply( tf.linalg.diag_part(self.H), tf.math.multiply( U, V ) )\n",
        "            \n",
        "            # W = alpha/den\n",
        "            return( tf.math.divide( self.alpha, den ) )        \n",
        "\n",
        "\n",
        "        def gcn(self, name):\n",
        "            # 2 Layers\n",
        "            L = 2\n",
        "            \n",
        "            # Hidden dim = 5\n",
        "            input_dim = [self.feature_dim,5]\n",
        "            output_dim = [5,1]\n",
        "            \n",
        "            ## NSI [Batch_size X Nodes X Features]\n",
        "            x = tf.ones([self.batch_size, self.nNodes, 1], dtype=tf.float64)\n",
        "            #x = self.x\n",
        "                        \n",
        "            with tf.compat.v1.variable_scope('gcn_'+name):\n",
        "                for l in range(L):\n",
        "                    with tf.compat.v1.variable_scope('gc_l{}'.format(l+1)):\n",
        "                        # Weights\n",
        "                        w1 = tf.compat.v1.get_variable( name='w1', shape=(input_dim[l], output_dim[l]), initializer=tf.initializers.glorot_uniform(), dtype=tf.float64)\n",
        "                        w0 = tf.compat.v1.get_variable( name='w0', shape=(input_dim[l], output_dim[l]), initializer=tf.initializers.glorot_uniform(), dtype=tf.float64)\n",
        "                        \n",
        "                        ## Biases\n",
        "                        b1 = tf.compat.v1.get_variable( name='b1', initializer=tf.constant(0.1, shape=(output_dim[l],), dtype=tf.float64) )\n",
        "                        b0 = tf.compat.v1.get_variable( name='b0', initializer=tf.constant(0.1, shape=(output_dim[l],), dtype=tf.float64) )\n",
        "                        \n",
        "                        # XW\n",
        "                        x1 = tf.matmul(x, w1)\n",
        "                        x0 = tf.matmul(x, w0)\n",
        "                        \n",
        "                        # diag(A)XW0 + AXW1\n",
        "                        x1 = tf.matmul(self.H, x1)  \n",
        "                        x0 = tf.matmul(self.dH, x0)\n",
        "                        \n",
        "                        ## AXW + B\n",
        "                        x1 = tf.add(x1, b1)\n",
        "                        x0 = tf.add(x0, b0)\n",
        "                        \n",
        "                        # Combine\n",
        "                        x = x1 + x0\n",
        "                        \n",
        "                        # activation(AXW + B)\n",
        "                        if l == 0:\n",
        "                            x = tf.nn.relu(x)  \n",
        "                        else:\n",
        "                            x = tf.nn.sigmoid(x)\n",
        "\n",
        "                # Coefficients (a / b) [Batch_size X Nodes]\n",
        "                output = tf.squeeze(x)\n",
        "                #print(name)\n",
        "                #output\n",
        "            \n",
        "            return output\n",
        "        \n",
        "        def V_block(self, U, W, mu):\n",
        "            # H_ii * u_i * w_i\n",
        "            num = tf.math.multiply( tf.linalg.diag_part(self.H), tf.math.multiply( U, W ) )\n",
        "            \n",
        "            # mu + sum_j( (H_ij)^2 * (u_j)^2 *w_j )\n",
        "            den = tf.math.add( tf.reshape( tf.matmul( self.Hsq, tf.reshape( tf.math.multiply( tf.math.square( U ), W ), [-1, self.nNodes, 1] ) ), [-1, self.nNodes] ), mu)\n",
        "            \n",
        "            # V = num/den\n",
        "            return( tf.math.divide( num, den ) )        \n",
        "                                                                                \n",
        "        def build_objective(self):\n",
        "            # (H_ii)^2 * (v_i)^2\n",
        "            num = tf.math.multiply( tf.linalg.diag_part(self.Hsq), tf.math.square( self.pow_alloc ) )\n",
        "            \n",
        "            # sigma^2 + sum_j j ~= i ( (H_ji)^2 * (v_j)^2 ) \n",
        "            den = tf.reshape( tf.matmul( tf.transpose( self.Hsq, perm=[0,2,1] ), tf.reshape( tf.math.square( self.pow_alloc ), [-1, self.nNodes, 1] ) ), [-1, self.nNodes] ) + self.var - num \n",
        "            \n",
        "            # rate\n",
        "            rate = tf.math.log( 1. + tf.math.divide( num, den ) ) / tf.cast( tf.math.log( 2.0 ), tf.float64 )\n",
        "            \n",
        "            # Sum Rate = sum_i ( log(1 + SINR) )\n",
        "            self.utility = tf.reduce_sum( rate, axis=1 )\n",
        "            \n",
        "            # Weighted Sum Rate\n",
        "            #rate = tf.math.multiply( self.alpha, rate )\n",
        "            #self.utility = tf.reduce_sum( rate, axis=1 )\n",
        "            \n",
        "            # Minimization objective\n",
        "            self.obj = -tf.reduce_mean( self.utility )\n",
        "            \n",
        "            if self.exp == 'uwmmse':\n",
        "                self.init_optimizer()\n",
        "\n",
        "        def init_optimizer(self):\n",
        "            # Gradients and SGD update operation for training the model\n",
        "            self.trainable_params = tf.compat.v1.trainable_variables()\n",
        "            \n",
        "            #Learning Rate Decay\n",
        "            #starter_learning_rate = self.learning_rate\n",
        "            #self.learning_rate_decayed = tf.train.exponential_decay(starter_learning_rate, global_step=self.global_step, decay_steps=5000, decay_rate=0.99, staircase=True)\n",
        "            \n",
        "            # SGD with Momentum\n",
        "            #self.opt = tf.train.GradientDescentOptimizer( learning_rate=learning_rate )\n",
        "            #self.opt = tf.train.MomentumOptimizer(learning_rate=self.learning_rate_decayed, momentum=0.9, use_nesterov=True )\n",
        "\n",
        "            # Adam Optimizer\n",
        "            self.opt = tf.compat.v1.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "\n",
        "            # Compute gradients of loss w.r.t. all trainable variables\n",
        "            gradients = tf.gradients(self.obj, self.trainable_params)\n",
        "\n",
        "            # Clip gradients by a given maximum_gradient_norm\n",
        "            clip_gradients, _ = tf.clip_by_global_norm(gradients, self.max_gradient_norm)\n",
        "            \n",
        "            # Update the model\n",
        "            self.updates = self.opt.apply_gradients(\n",
        "                zip(clip_gradients, self.trainable_params), global_step=self.global_step)\n",
        "                \n",
        "        def save(self, sess, path, var_list=None, global_step=None):\n",
        "            saver = tf.compat.v1.train.Saver(var_list)\n",
        "            save_path = saver.save(sess, save_path=path, global_step=global_step)\n",
        "\n",
        "        def restore(self, sess, path, var_list=None):\n",
        "            saver = tf.compat.v1.train.Saver(var_list)\n",
        "            saver.restore(sess, save_path=tf.train.latest_checkpoint(path))\n",
        "\n",
        "        def train(self, sess, inputs ):\n",
        "            input_feed = dict()\n",
        "            input_feed[self.H.name] = inputs\n",
        "            #input_feed[self.x.name] = features\n",
        "            #input_feed[self.alpha.name] = alpha\n",
        "            \n",
        "            # Training Phase\n",
        "            #input_feed[self.phase.name] = True\n",
        " \n",
        "            output_feed = [self.obj, self.utility, self.pow_alloc, self.updates]\n",
        "                            \n",
        "            outputs = sess.run(output_feed, input_feed)\n",
        "            \n",
        "            return outputs[0], outputs[1], outputs[2]\n",
        "\n",
        "\n",
        "        def eval(self, sess, inputs ):\n",
        "            input_feed = dict()\n",
        "            input_feed[self.H.name] = inputs\n",
        "            #input_feed[self.x.name] = features\n",
        "            #input_feed[self.alpha.name] = alpha\n",
        "\n",
        "            # Training Phase\n",
        "            #input_feed[self.phase.name] = False\n",
        "\n",
        "            output_feed = [self.obj,self.utility, self.pow_alloc] \n",
        "                           \n",
        "            outputs = sess.run(output_feed, input_feed)\n",
        "            \n",
        "            return outputs[0], outputs[1], outputs[2]\n"
      ],
      "metadata": {
        "id": "jfjXIaBt8iKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MAIN "
      ],
      "metadata": {
        "id": "CxNZoUp_mHD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "argv_3 = 'train'\n",
        "argv_2 = 'uwmmse'\n",
        "argv_1 = 'nNode_25_pl_2p2'"
      ],
      "metadata": {
        "id": "-Igg29XvrXQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "# Imports\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "import sys\n",
        "import pdb\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
        "\n",
        "\n",
        "# Experiment \n",
        "dataID = argv_1\n",
        "exp = argv_2\n",
        "mode = argv_3\n",
        "if len( sys.argv ) > 3:\n",
        "    #mode = sys.argv[3]\n",
        "    mode = argv_3\n",
        "\n",
        "# Maximum available power at each node\n",
        "Pmax = 1.0\n",
        "\n",
        "# Noise power\n",
        "var_db = -91\n",
        "var = 10**(var_db/10)\n",
        "\n",
        "# Features\n",
        "feature_dim = 1\n",
        "\n",
        "# Batch size\n",
        "batch_size = 64\n",
        "\n",
        "# Layers UWMMSE = 4 (default)  WMMSE = 100 (default)\n",
        "layers = 4 if exp == 'uwmmse' else 100\n",
        "\n",
        "# Learning rate\n",
        "learning_rate=1e-3\n",
        "\n",
        "# Number of epochs\n",
        "nEpoch = 20\n",
        "\n",
        "    \n",
        "# Create Model Instance\n",
        "def create_model( session, exp='uwmmse' ):\n",
        "    # Create\n",
        "    model = UWMMSE( Pmax=Pmax, var=var, feature_dim=feature_dim, batch_size=batch_size, layers=layers, learning_rate=learning_rate, exp=exp )\n",
        "    \n",
        "    # Initialize variables ( To train from scratch )\n",
        "    session.run(tf.compat.v1.global_variables_initializer())\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Train\n",
        "def mainTrain():        \n",
        "    # Data\n",
        "    # uncomment this on your first runs. This line loads the dataset. If your dataset loads multiple times, the ram will overflow\n",
        "    H = pickle.load( open( '/content/gdrive/My Drive/THESIS DATASETS/SISO_DATASET/'+dataID+'/H.pkl', 'rb' ) )\n",
        "    \n",
        "    #Training data\n",
        "    train_H = H['train_H']\n",
        "    \n",
        "    #Test data\n",
        "    test_H = H['test_H']\n",
        "    \n",
        "    # Initiate TF session\n",
        "    with tf.compat.v1.Session(config=config) as sess:\n",
        "    \n",
        "        # WMMSE experiment\n",
        "        if exp == 'wmmse':\n",
        "        \n",
        "            # Create model \n",
        "            model = create_model( sess, exp )\n",
        "            \n",
        "            # Test\n",
        "            test_iter = len(test_H)\n",
        "                    \n",
        "            print( '\\nWMMSE Started\\n' )\n",
        "\n",
        "            t = 0.\n",
        "            test_rate = 0.0\n",
        "            sum_rate = []\n",
        "            \n",
        "            for batch in range(test_iter):\n",
        "                #print(batch)\n",
        "                batch_test_inputs = test_H[batch]                \n",
        "                start = time.time()\n",
        "                avg_rate, batch_rate, batch_power = model.eval( sess, inputs=batch_test_inputs )\n",
        "                t += (time.time() - start)\n",
        "                test_rate += -avg_rate\n",
        "                sum_rate.append( batch_rate )\n",
        "                \n",
        "            test_rate /= test_iter\n",
        "\n",
        "            # Average per-iteration test time\n",
        "            t = t / test_iter\n",
        "            \n",
        "            log = \"Test_rate = {:.3f}, Time = {:.3f} sec\\n\"\n",
        "            print(log.format( test_rate, t))\n",
        "            \n",
        "        # Unrolled WMMSE experiment\n",
        "        else:\n",
        "            \n",
        "            # Create model \n",
        "            model = create_model( sess )\n",
        "\n",
        "            if mode == 'train':\n",
        "                # Create model path\n",
        "                if not os.path.exists('models/'+dataID):\n",
        "                    os.makedirs('models/'+dataID)\n",
        "                    \n",
        "                #Training loop\n",
        "                print( '\\nUWMMSE Training Started\\n' )\n",
        "                max_rate = 0.\n",
        "                train_iter = len(train_H)\n",
        "                \n",
        "                #nEpoch = 1\n",
        "                for epoch in range(nEpoch):\n",
        "                    start = time.time()\n",
        "                    train_rate = 0.0\n",
        "                \n",
        "                    for it in range(train_iter):\n",
        "                        batch_train_inputs = train_H[it]\n",
        "                        step_rate, batch_rate, power  = model.train( sess, inputs=batch_train_inputs ) \n",
        "                        if np.isnan(step_rate) or np.isinf(step_rate) :\n",
        "                            pdb.set_trace()\n",
        "                        train_rate += -step_rate\n",
        "                    train_rate /= train_iter\n",
        "                    \n",
        "                    log = \"Epoch {}/{}, Average Sum_rate = {:.3f}, Time = {:.3f} sec\\n\"\n",
        "                    print(log.format( epoch+1, nEpoch, train_rate, time.time() - start) )\n",
        "                    \n",
        "                    # Save model with best average sum-rate\n",
        "                    if train_rate > max_rate:\n",
        "                        max_rate = train_rate\n",
        "                        model.save(sess, path='models/'+dataID+'/uwmmse-model', global_step=(epoch+1))\n",
        "                    \n",
        "                    # Shuffle\n",
        "                    shuffled_indices = np.random.permutation(train_iter)\n",
        "                    train_H = [train_H[indx] for indx in shuffled_indices]\n",
        "\n",
        "                    \n",
        "                print( 'Training Complete' )\n",
        "\n",
        "            # Test\n",
        "            test_iter = len(test_H)       \n",
        "\n",
        "            # Restore best saved model\n",
        "            model.restore(sess, path='models/'+dataID+'/')\n",
        "\n",
        "            print( '\\nUWMMSE Testing Started\\n' )\n",
        "\n",
        "            t = 0.\n",
        "            test_rate = 0.0\n",
        "            sum_rate = []\n",
        "\n",
        "            for batch in range(test_iter):\n",
        "                batch_test_inputs = test_H[batch]\n",
        "                start = time.time()\n",
        "                avg_rate, batch_rate, batch_power = model.eval( sess, inputs=batch_test_inputs )\n",
        "                if np.isnan(avg_rate) or np.isinf(avg_rate):\n",
        "                    pdb.set_trace()\n",
        "                t += (time.time() - start)\n",
        "                sum_rate.append( batch_rate )\n",
        "                test_rate += -avg_rate\n",
        "            \n",
        "            \n",
        "            test_rate /= test_iter\n",
        "            \n",
        "            ## Average per-iteration test time   \n",
        "            t = t / test_iter\n",
        "\n",
        "            log = \"Test_rate = {:.3f}, Time = {:.3f} sec\\n\"\n",
        "            print(log.format( test_rate, t))\n",
        "            #print(sum_rate)\n",
        "\n",
        "            \n",
        "if __name__ == \"__main__\":        \n",
        "    import sys\n",
        "\n",
        "    rn = np.random.randint(2**20)\n",
        "    rn1 = np.random.randint(2**20)\n",
        "    tf.random.set_seed(rn)\n",
        "    np.random.seed(rn1)\n",
        "\n",
        "    mainTrain()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l65-udD5mMua",
        "outputId": "7767784b-2b14-436d-fc68-b58ed9b8bc0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "UWMMSE Training Started\n",
            "\n",
            "Epoch 1/20, Average Sum_rate = 106.110, Time = 112.621 sec\n",
            "\n",
            "Epoch 2/20, Average Sum_rate = 108.026, Time = 101.393 sec\n",
            "\n",
            "Epoch 3/20, Average Sum_rate = 108.185, Time = 101.225 sec\n",
            "\n",
            "Epoch 4/20, Average Sum_rate = 108.234, Time = 101.240 sec\n",
            "\n",
            "Epoch 5/20, Average Sum_rate = 108.268, Time = 99.581 sec\n",
            "\n",
            "Epoch 6/20, Average Sum_rate = 108.304, Time = 100.667 sec\n",
            "\n",
            "Epoch 7/20, Average Sum_rate = 108.333, Time = 105.050 sec\n",
            "\n",
            "Epoch 8/20, Average Sum_rate = 108.353, Time = 100.902 sec\n",
            "\n",
            "Epoch 9/20, Average Sum_rate = 108.369, Time = 100.143 sec\n",
            "\n",
            "Epoch 10/20, Average Sum_rate = 108.386, Time = 99.502 sec\n",
            "\n",
            "Epoch 11/20, Average Sum_rate = 108.406, Time = 102.253 sec\n",
            "\n",
            "Epoch 12/20, Average Sum_rate = 108.428, Time = 102.061 sec\n",
            "\n",
            "Epoch 13/20, Average Sum_rate = 108.452, Time = 102.242 sec\n",
            "\n",
            "Epoch 14/20, Average Sum_rate = 108.477, Time = 100.848 sec\n",
            "\n",
            "Epoch 15/20, Average Sum_rate = 108.503, Time = 100.537 sec\n",
            "\n",
            "Epoch 16/20, Average Sum_rate = 108.527, Time = 99.974 sec\n",
            "\n",
            "Epoch 17/20, Average Sum_rate = 108.556, Time = 102.555 sec\n",
            "\n",
            "Epoch 18/20, Average Sum_rate = 108.581, Time = 102.122 sec\n",
            "\n",
            "Epoch 19/20, Average Sum_rate = 108.597, Time = 102.344 sec\n",
            "\n",
            "Epoch 20/20, Average Sum_rate = 108.609, Time = 102.907 sec\n",
            "\n",
            "Training Complete\n",
            "INFO:tensorflow:Restoring parameters from models/nNode_25_pl_2p2/uwmmse-model-20\n",
            "\n",
            "UWMMSE Testing Started\n",
            "\n",
            "Test_rate = 108.735, Time = 0.007 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# just uwmmse test"
      ],
      "metadata": {
        "id": "BuySF6uz0MQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "amTaa4gaNp5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# When you need to clear it all and make another run"
      ],
      "metadata": {
        "id": "SOQCXo8-QQ_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sys.modules[__name__].__dict__.clear()"
      ],
      "metadata": {
        "id": "untbB1KdQWY9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}